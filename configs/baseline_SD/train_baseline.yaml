# RoentGen V4: Demographic Encoder Training Configuration
#
# V4 uses a lightweight demographic encoder that separates demographic conditioning
# from clinical text conditioning. This avoids CLIP's 77-token bottleneck and enables
# clean separation of concerns.
#
# Key differences from V2/V3:
# - No HCN hierarchical composition (simpler embeddings + MLP)
# - Demographics encoded as categorical indices, not text
# - No CLIP modification (keeps pre-trained positional embeddings)
# - Strong auxiliary supervision (weight=1.0) to force meaningful embeddings
# - Optional demographic dropout to force UNet to use the encoder
#
# ==============================================================================
# MODEL CONFIGURATION
# ==============================================================================

pretrained_model_name_or_path: "stabilityai/stable-diffusion-2-1-base"
pretrained_text_encoder_name_or_path: null
train_text_encoder: True

embedding_method: "last_hidden_state"
enforce_tokenizer_max_sentence_length: 77
output_dir: "./outputs/baseline SD/train_baseline"
seed: 42
resolution: 512

train_batch_size: 16
max_train_steps: 30000
learning_rate: 1.0e-5
gradient_accumulation_steps: 1
mixed_precision: "bf16"
use_ema: true
lr_scheduler: "cosine"
lr_warmup_steps: 500
gradient_checkpointing: true

# ==============================================================================
# DATASET CONFIGURATION
# ==============================================================================

use_wds_dataset: true
url_root: "./demo_chest/training_data"
image_type: "pt"

# ==============================================================================
# TEST DATASET Generation
# ==============================================================================


wds_dataset_path: "./demo_chest/training_data"
num_images_per_prompt: 1
generation_batch_size: 16
generation_checkpoint_path: null
# ==============================================================================
# CHECKPOINTING
# ==============================================================================

checkpointing_steps: 500
checkpoints_total_limit: None
resume_from_checkpoint: "latest"


# ==============================================================================
# STRIP DEMOGRAPHICS
# ==============================================================================

strip_demographics: false

# ==============================================================================
# HCN (DISABLED FOR V4)
# ==============================================================================
# V4 uses DemographicEncoder instead of HCN
use_hcn: false
hcn_d_node: 256
hcn_d_ctx: 1024
hcn_dropout: 0.1
hcn_use_uncertainty: true
hcn_num_age_bins: 5
hcn_num_sex: 2
hcn_num_race: 4
hcn_kl_weight: 0.005
hcn_kl_anneal_steps: 5000
hcn_comp_weight: 0.01
hcn_aux_weight: 0

# ==============================================================================
# DEMOGRAPHIC ENCODER (V4)
# ==============================================================================

# Enable DemographicEncoder for clean demographic conditioning
use_demographic_encoder: false

# Architecture parameters
demo_d_hidden: 256              # Hidden dimension for embeddings
demo_d_output: 1024             # Output dimension (matches text encoder)
demo_dropout: 0.1               # Dropout in MLP

# Number of categories for each demographic attribute
demo_num_age_bins: 5            # Age bins: [0-18, 18-40, 40-60, 60-80, 80+]
demo_num_sex: 2                 # Sex: [Male, Female]
demo_num_race: 4                # Race: [White, Black, Asian, Hispanic]

# Loss weights
demo_aux_weight: 1.0            # Strong auxiliary loss to force meaningful embeddings

# Dropout strategy (optional - set to false for more stable training)
demo_use_dropout: false         # Whether to randomly remove demographics from text
demo_text_dropout_prob: 0    # Probability of removing demographics (if enabled)
demo_dropout_start_step: 0      # Step to start dropout (0 = from beginning)

# Optional: Path to pretrained demographic encoder
demographic_encoder_pretrained_path: null

# ==============================================================================
# VALIDATION CONFIGURATION
# ==============================================================================

run_validation: true
validation_steps: 2500
validation_schedule_offsets:  # . For example, with validation_steps: 2500 and validation_schedule_offsets: [-1500, 0], the monitor checks steps {2500*k - 1500, 2500*k} â†’ 1000, 2500, 3500, 5000, etc.
  - -1500
  - 0
num_validation_samples: -1
val_batch_size: 8
validation_csv: null
validation_images_dir: "./demo_chest/val_data"
validation_sex_model_path: "./pretrained_models/sex/resnet-all/epoch=13-step=7125.ckpt"
validation_num_images_per_prompt: 1
validation_guidance_scale: 7.5
validation_num_inference_steps: 75
validation_save_images: true
validation_metrics_batch_size: 8
compute_subgroup_metrics: true
strip_demographics_in_validation: false


# ==============================================================================
# VALIDATION CONFIGURATION
# ==============================================================================


test_csv: "./demo_chest/test_data"
test_images_dir: "./demo_chest/test_data"
test_dir: "./demo_chest/test_data"
train_dir: "./demo_chest/training_data"

# ==============================================================================
# GROUND TRUTH (GT) DATASET CONFIGURATION
# ==============================================================================
# Configuration for generating synthetic images from GT dataset
# When run in gt_mode, this will use GT_data as the source dataset

GT_dir: null
gt_dir: null
# ==============================================================================
# LOGGING
# ==============================================================================

logging_dir: "logs_summarized"
report_to: "wandb"


